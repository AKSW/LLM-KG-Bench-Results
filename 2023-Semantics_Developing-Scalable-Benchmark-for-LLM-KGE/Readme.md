# Results of LLM-KG-Bench runs SEMANTICS 2023 poster track

Results and log of [LLM-KG-Bench](https://github.com/AKSW/LLM-KG-Bench) runs described in article "Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering", Meyer et al., to appear in [SEMANTICS 2023 poster track](https://2023-eu.semantics.cc/page/accepted_posters) proceedings.

We collected data in multiple runs, each resulting in files with date and time of experiment start in filename:
* result files (`.json`, `.txt`, `.yaml`, same info in different serializations) containing task, response and evaluation data
* model log files (`.jsonnl` in directory `modelLog`) containing details on LLM interaction
* full log files (`.log` in directory `log`) containing debug log for runs
